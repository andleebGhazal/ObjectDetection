{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dBQDGpjpbBw",
        "outputId": "cf930fb7-c1de-442a-eedd-9d01b85e2ac2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB7uPkag8k_F"
      },
      "source": [
        "Access files and folders stored in your Google Drive within your Colab notebook by referencing the path '/content/drive/'. This allows you to read, write, or manipulate these files using Python code in the Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aSuYMi7wpgb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxCa_IM_9nFp"
      },
      "source": [
        "imported the necessary libraries and modules required for handling image data, creating the neural network architecture, and setting up data augmentation techniques for training model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6znVDlchxfoQ"
      },
      "outputs": [],
      "source": [
        "# Assuming these are the image dimensions\n",
        "image_height = 224\n",
        "image_width = 224\n",
        "\n",
        "# Load data\n",
        "base_path_images = '/content/drive/My Drive/dataset/My Game Pics.v5i.tensorflow/train'\n",
        "path_to_csv = '/content/drive/My Drive/dataset/My Game Pics.v5i.tensorflow/train/_annotations.csv'\n",
        "df = pd.read_csv(path_to_csv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_aiShtY-LE0"
      },
      "source": [
        "df will contain the data from the CSV file, which can be further process and use to link annotations or labels to the corresponding images in dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gl3eTO8Gxrj0"
      },
      "outputs": [],
      "source": [
        "# Split the data into Train, Validation, and Test sets\n",
        "train_data, test_data = train_test_split(df, test_size=0.15, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhH1eb1O-xbp"
      },
      "source": [
        " splitting the dataset into training, validation, and test sets using the train_test_split function twice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnm5Yoyg-ukV"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83oygsmsxrZY"
      },
      "outputs": [],
      "source": [
        "def load_images_and_labels(data):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for _, row in data.iterrows():\n",
        "        img_filename = row['filename']\n",
        "        img_path = os.path.join(base_path_images, img_filename)\n",
        "        image = cv2.imread(img_path)\n",
        "        if image is not None:\n",
        "            image = cv2.resize(image, (image_width, image_height))\n",
        "            images.append(image)\n",
        "            labels.append([row['xmin'], row['ymin'], row['xmax'], row['ymax']])\n",
        "    return np.array(images), np.array(labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj9MXocm_jPa"
      },
      "source": [
        "This function essentially reads images from the file paths provided in the dataset, resizes them to a standardized dimension, and collects their corresponding bounding box coordinates for further processing in machine learning pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26VxkgQgxrO8"
      },
      "outputs": [],
      "source": [
        "# Data augmentation setup\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,  # Normalizing pixel values between 0 and 1\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq20fkp1_9_F"
      },
      "source": [
        "These generators are useful for training deep learning models by augmenting and normalizing the input images. They facilitate better generalization and learning by exposing the model to various transformations and ensuring consistent input data scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ap8O_BCFxrCs"
      },
      "outputs": [],
      "source": [
        "train_images, train_labels = load_images_and_labels(train_data)\n",
        "val_images, val_labels = load_images_and_labels(val_data)\n",
        "test_images, test_labels = load_images_and_labels(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgtyZauLBvYc"
      },
      "source": [
        "Each of sets of variables (_images and _labels) holds the image data and their respective annotations or labels, which will be used during the model training, validation, and evaluation stages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7GzC7U2cxqn3"
      },
      "outputs": [],
      "source": [
        "# Define your CNN model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the feature extractor (backbone architecture)\n",
        "model.add(Conv2D(filters=32, kernel_size=(7, 7), activation='relu', input_shape=(image_height, image_width, 3)))\n",
        "model.add(Conv2D(filters=32, kernel_size=(9, 9), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))  # Adding MaxPooling layer\n",
        "model.add(Conv2D(filters=64, kernel_size=(7, 7), activation='relu'))\n",
        "model.add(Conv2D(filters=64, kernel_size=(9, 9), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))  # Adding MaxPooling layer\n",
        "model.add(Conv2D(filters=64, kernel_size=(9, 9), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))  # Adding MaxPooling layer\n",
        "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The defined layers form the backbone architecture of the CNN for feature extraction."
      ],
      "metadata": {
        "id": "KvB5LqKre_Uc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5L166kN2xpsK",
        "outputId": "c2ef7214-d905-49bb-8b5e-c7c737a9c319"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 218, 218, 32)      4736      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 210, 210, 32)      82976     \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 105, 105, 32)      0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 99, 99, 64)        100416    \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 91, 91, 64)        331840    \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 45, 45, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 37, 37, 64)        331840    \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 18, 18, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 16, 16, 64)        256       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 16384)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               2097280   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 516       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2986788 (11.39 MB)\n",
            "Trainable params: 2986660 (11.39 MB)\n",
            "Non-trainable params: 128 (512.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.add(BatchNormalization())  # Batch normalization\n",
        "\n",
        "# Flatten and add Dense layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "model.add(Dense(units=4, activation='linear'))  # Output layer for bounding box coordinates\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "09TsW7q7x92f"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error',metrics=['accuracy'])  # Use appropriate loss function for bounding box regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMneBhI0x9sp",
        "outputId": "31cfacdb-c170-4828-8b60-8f8a6b872d07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "229/271 [========================>.....] - ETA: 29:03 - loss: 81659.9219 - accuracy: 0.6373"
          ]
        }
      ],
      "source": [
        "# Train the model using images and labels with data augmentation\n",
        "history = model.fit(train_images, train_labels, epochs=100, validation_data=(test_images, test_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A58Z3DCPycP8",
        "outputId": "5f60bc08-9233-4767-f077-95acb8a90ca0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "57/57 [==============================] - 4s 69ms/step - loss: 63975.7656\n",
            "Test Loss: 63975.765625\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on test data\n",
        "loss = model.evaluate(test_datagen.flow(test_images, test_labels))\n",
        "\n",
        "print(f\"Test Loss: {loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3H-pO9WEg9x"
      },
      "outputs": [],
      "source": [
        "# Perform predictions using the trained model\n",
        "batch_size = 32\n",
        "num_test_samples = len(test_images)\n",
        "predictions = []\n",
        "\n",
        "for i in range(0, num_test_samples, batch_size):\n",
        "    batch_images = test_images[i:i+batch_size]\n",
        "    batch_predictions = model.predict(test_datagen.flow(batch_images, batch_size=batch_size))\n",
        "    predictions.extend(batch_predictions)\n",
        "\n",
        "# Convert predictions to a numpy array\n",
        "predictions = np.array(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5A3_Qs5yb9u"
      },
      "outputs": [],
      "source": [
        "# Compute IoU (Intersection over Union) for each predicted bounding box\n",
        "def calculate_iou(y_true, y_pred):\n",
        "    # Extract coordinates\n",
        "    true_xmin, true_ymin, true_xmax, true_ymax = y_true\n",
        "    pred_xmin, pred_ymin, pred_xmax, pred_ymax = y_pred\n",
        "\n",
        "    # Calculate intersection coordinates\n",
        "    xmin = max(true_xmin, pred_xmin)\n",
        "    ymin = max(true_ymin, pred_ymin)\n",
        "    xmax = min(true_xmax, pred_xmax)\n",
        "    ymax = min(true_ymax, pred_ymax)\n",
        "\n",
        "    # Calculate area of intersection rectangle\n",
        "    intersection_area = max(0, xmax - xmin) * max(0, ymax - ymin)\n",
        "\n",
        "    # Calculate areas of true and predicted rectangles\n",
        "    true_area = (true_xmax - true_xmin) * (true_ymax - true_ymin)\n",
        "    pred_area = (pred_xmax - pred_xmin) * (pred_ymax - pred_ymin)\n",
        "\n",
        "    # Calculate Union area\n",
        "    union_area = true_area + pred_area - intersection_area\n",
        "\n",
        "    # Avoid division by zero\n",
        "    epsilon = 1e-5\n",
        "\n",
        "    # Calculate IoU\n",
        "    iou = intersection_area / (union_area + epsilon)\n",
        "    return iou"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "he function returns the computed IoU, which quantifies the overlap between the predicted and true bounding boxes. Higher IoU values indicate better overlap and accuracy of the predicted bounding box concerning the true one."
      ],
      "metadata": {
        "id": "xI9KQVOOjpLp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGjZgU_zyb0t"
      },
      "outputs": [],
      "source": [
        "iou_scores = []\n",
        "for i in range(len(test_images)):\n",
        "    true_bbox = test_labels[i]\n",
        "    pred_bbox = predictions[i]\n",
        "    iou = calculate_iou(true_bbox, pred_bbox)\n",
        "    iou_scores.append(iou)\n",
        "\n",
        "# Calculate mean IoU\n",
        "mean_iou = np.mean(iou_scores)\n",
        "print(f\"Mean IoU: {mean_iou}\")\n",
        "\n",
        "# Plotting accuracy\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# Plotting loss\n",
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mean IoU gives an overall measure of the model's accuracy in predicting bounding boxes"
      ],
      "metadata": {
        "id": "Dznv6qhnm9Ny"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-Ocsb5LybsS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}